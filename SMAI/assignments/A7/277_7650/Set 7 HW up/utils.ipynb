{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJ7eJWxqFJhA"
   },
   "outputs": [],
   "source": [
    "import gif\n",
    "import seaborn as sns; sns.set();\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YnKpXSgwFR7j"
   },
   "outputs": [],
   "source": [
    "# Function to plot our input data for classification tasks.\n",
    "def plot_2D_input_datapoints(X_inp, y_inp):\n",
    "\n",
    "  \"\"\"Method to plot 2D datapoints for classification tasks.\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "\n",
    "  X_inp: ndarray (num_examples(rows) vs num_features(columns))\n",
    "    Input data which would be plotted.\n",
    "\n",
    "  Y_inp: ndarray (num_examples(rows) vs num_outputs(columns))\n",
    "    Corresponding labels of X_inp\n",
    "  \"\"\"\n",
    "  sns.set();\n",
    "  \n",
    "  X_inp = X_inp[:, :2]\n",
    "  inp_data = np.hstack((X_inp, y_inp.reshape(-1, 1)))\n",
    "  df = pd.DataFrame(data=inp_data, columns=[\"$X_0$\", \"$X_1$\", \"$y$\"])\n",
    "  sns.scatterplot(x=\"$X_0$\", y=\"$X_1$\", hue=\"$y$\", legend='full',  data=df)\n",
    "  plt.title('Input data')\n",
    "  plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4X2i_xWVseyi"
   },
   "outputs": [],
   "source": [
    "# Sourced from https://github.com/eriklindernoren/ML-From-Scratch\n",
    "# Method to normalize the data\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSXAuG5usna3"
   },
   "outputs": [],
   "source": [
    "# Defining our sigmoid activation function\n",
    "def sigmoid(vec_w_x, predict='no'):\n",
    "\n",
    "  \"\"\" Sigmoid activation for binary classification.\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "   \n",
    "  predict: str ('yes' or 'no')\n",
    "    'no' corresponds to training\n",
    "    'yes' corresponds to prediction\n",
    "  \"\"\"\n",
    "\n",
    "  pred_prob = (1 / (1 + np.exp(-1 * vec_w_x)))\n",
    "  \n",
    "  if predict == 'yes':\n",
    "    pred_prob[pred_prob >= 0.5] = 1\n",
    "    pred_prob[pred_prob < 0.5] = 0\n",
    "\n",
    "  return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoH4TX3p7uqS"
   },
   "outputs": [],
   "source": [
    "# Defining our softmax function for classification\n",
    "def softmax(X_input_set, vec_w_x, predict='no'):\n",
    "\n",
    "  \"\"\" softmax activation for multiclass classification.\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "  \n",
    "  X_input_set: ndarray\n",
    "    Input dataset whose examples have to be classified\n",
    "  \n",
    "  predict: str ('yes' or 'no')\n",
    "    'no' corresponds to training\n",
    "    'yes' corresponds to prediction\n",
    "  \"\"\"\n",
    "\n",
    "  num_datapoints = np.shape(X_input_set)[0]\n",
    "\n",
    "  # Computing predicted probabilites using softmax function\n",
    "  exp_units = np.exp(vec_w_x)\n",
    "  summation_exp_units = np.sum(exp_units, axis=-1, keepdims=True)\n",
    "  pred_prob = exp_units/summation_exp_units\n",
    "\n",
    "  # Predicting classes\n",
    "  if predict == 'yes':\n",
    "    maxvals = np.amax(pred_prob, axis=1)\n",
    "    for i in range(num_datapoints):\n",
    "      idx = np.argwhere(pred_prob == maxvals[i])[0]\n",
    "      pred_prob[idx[0], idx[1]] = 1\n",
    "\n",
    "    non_maxvals_idxs = np.argwhere(pred_prob != 1)\n",
    "    pred_prob[non_maxvals_idxs[:, 0], non_maxvals_idxs[:, 1]] = 0\n",
    "\n",
    "  return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pU-o8-g-iuYp"
   },
   "outputs": [],
   "source": [
    "# Defining signum activation function\n",
    "def signum(vec_w_x):\n",
    "  \"\"\" signum activation for perceptron\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "  \"\"\"\n",
    "\n",
    "  vec_w_x[vec_w_x >= 0] = 1\n",
    "  vec_w_x[vec_w_x < 0] = -1\n",
    "  return vec_w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEyGBzdkmS0p"
   },
   "outputs": [],
   "source": [
    "# multi-class signum\n",
    "def multi_class_signum(vec_w_x):\n",
    "  \"\"\" Multiclass signum activation.\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "  \"\"\"\n",
    "\n",
    "  flag = np.all(vec_w_x == 0)\n",
    "\n",
    "  if flag:\n",
    "    return vec_w_x\n",
    "\n",
    "  else:\n",
    "    num_examples, num_outputs = np.shape(vec_w_x)\n",
    "    range_examples = np.array(range(0, num_examples))\n",
    "\n",
    "    zero_idxs = np.argwhere(np.all(vec_w_x == 0, axis=1))\n",
    "    non_zero_examples = np.delete(range_examples, zero_idxs[:, 0])\n",
    "      \n",
    "    signum_vec_w_x = vec_w_x[non_zero_examples]\n",
    "    maxvals = np.amax(signum_vec_w_x, axis=1)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "      idx = np.argwhere(signum_vec_w_x == maxvals[i])[0]\n",
    "      signum_vec_w_x[idx[0], idx[1]] = 1\n",
    "\n",
    "    non_maxvals_idxs = np.argwhere(signum_vec_w_x != 1)\n",
    "    signum_vec_w_x[non_maxvals_idxs[:, 0], non_maxvals_idxs[:, 1]] = -1\n",
    "    vec_w_x[non_zero_examples] = signum_vec_w_x\n",
    "\n",
    "    return vec_w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PS6QdyIBFHDc"
   },
   "outputs": [],
   "source": [
    "# Evaluation for train, val, and test set.\n",
    "def get_accuracy(y_predicted, Y_input_set, num_datapoints):\n",
    "  miscls_points = np.argwhere(np.any(y_predicted != Y_input_set, axis=1))\n",
    "  miscls_points = np.unique(miscls_points)\n",
    "  accuracy = (1-len(miscls_points)/num_datapoints)*100\n",
    "  return accuracy\n",
    "\n",
    "def get_prediction(X_input_set, Y_input_set, weights, get_acc=False, model_type='perceptron', predict='no'):\n",
    "\n",
    "  if len(Y_input_set) != 0:\n",
    "    num_datapoints, num_categories = np.shape(Y_input_set)\n",
    "\n",
    "  vec_w_transpose_x = np.dot(X_input_set, weights)\n",
    "\n",
    "  if num_categories > 1: # Multi-class\n",
    "    if model_type == 'perceptron':\n",
    "      y_pred_out = multi_class_signum(vec_w_transpose_x)\n",
    "    elif model_type == 'logreg':\n",
    "      y_pred_out = softmax(X_input_set, vec_w_transpose_x, predict=predict)\n",
    "\n",
    "  else: # Binary class\n",
    "    if model_type == 'perceptron' or model_type == 'LinearDA':\n",
    "      y_pred_out = signum(vec_w_transpose_x)\n",
    "    elif model_type == 'logreg':\n",
    "      y_pred_out = sigmoid(vec_w_transpose_x, predict=predict)\n",
    "\n",
    "  # Both prediction and evaluation\n",
    "  if get_acc:\n",
    "    cls_acc = get_accuracy(y_pred_out, Y_input_set, num_datapoints)\n",
    "    return cls_acc, y_pred_out\n",
    "  \n",
    "  # Only prediction\n",
    "  return y_pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0Xp96PxSjkB"
   },
   "outputs": [],
   "source": [
    "# Method to generate gifs of decision boundary\n",
    "def generate_gifs(X, y, trained_weights, dataset_type, path='/path/filename.gif', bias='on', class_label_01_form='off', model_type='perceptron', predict='no'):\n",
    "  if dataset_type == 'train':\n",
    "    frames = []\n",
    "    for i in range(len(trained_weights)):\n",
    "      frame = plot_decision_boundary(X, y, trained_weights, i, dataset_type=dataset_type, bias=bias, \n",
    "                                     class_label_01_form=class_label_01_form, model_type=model_type, predict=predict)\n",
    "      frames.append(frame)\n",
    "    gif.save(frames, path, duration=500)\n",
    "  \n",
    "  print('Gif/image generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE7IuGLt_yRb"
   },
   "outputs": [],
   "source": [
    "@gif.frame\n",
    "def plot_decision_boundary(X_input, Y_input, weights, i=0, dataset_type='test', bias='on', class_label_01_form='off', model_type='perceptron', predict='no'):\n",
    "\n",
    "  \"\"\" Plotting decision boundary for train/test set. This method can\n",
    "  also be used to plot a single decision boundary instead of a gif.\n",
    "\n",
    "  Parameters\n",
    "  -----------\n",
    "\n",
    "  X_input: ndarray (num_examples(rows) vs num_features(columns))\n",
    "    Input data which you would like to plot\n",
    "\n",
    "  Y_input: ndarray (num_examples(rows) vs num_outputs(columns))\n",
    "    Class labels\n",
    "\n",
    "  weights: ndarray (num_features(rows) vs num_outputs(columns))\n",
    "    Trained weights\n",
    "\n",
    "  dataset_type: str\n",
    "    Depending on the dataset_type, different plot/gifs will be generated.\n",
    "\n",
    "  bias: str\n",
    "    This is used to portray the effect of bias for the perceptron model\n",
    "\n",
    "  class_label_01_form: str\n",
    "    Signum activation results output labels in the form {-1, +1}.\n",
    "    If class_label_01_form is True, {-1, +1} will change to {0, 1}\n",
    "\n",
    "  model_type: str\n",
    "    Depending on the model_type, different plot/gifs will be generated.\n",
    "\n",
    "  predict: str\n",
    "    If predict is true, thresholding in activation function will be done.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure(figsize=(8,8));\n",
    "  sns.set();\n",
    "\n",
    "  h = 0.008\n",
    "\n",
    "  input_data = X_input[:,:2]\n",
    "  x0_min, x0_max = input_data[:, 0].min() - 1, input_data[:, 0].max() + 1\n",
    "  x1_min, x1_max = input_data[:, 1].min() - 1, input_data[:, 1].max() + 1\n",
    "  xx0, xx1 = np.mgrid[x0_min:x0_max:h, x1_min:x1_max:h]\n",
    "\n",
    "  # making prediction over the datapoint space\n",
    "  xx0_xx1 = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "  if bias == 'on':\n",
    "    b_ones = np.ones((len(xx0_xx1), 1))\n",
    "    xx0_xx1 = np.hstack((xx0_xx1, b_ones))\n",
    "\n",
    "  # Getting prediction over the datapoint space\n",
    "  if dataset_type == 'train':\n",
    "    #Z = get_prediction(xx0_xx1, np.array([]), weights[i], get_acc=False)\n",
    "    Z = np.dot(xx0_xx1, weights[i])\n",
    "    Z = Z.reshape(xx0.shape)\n",
    "    y_pred = get_prediction(X_input, Y_input, weights[i], get_acc=False, model_type=model_type, predict=predict)\n",
    "\n",
    "  elif dataset_type == 'test':\n",
    "    #Z = get_prediction(xx0_xx1, np.array([]), weights, get_acc=False)\n",
    "    Z = np.dot(xx0_xx1, weights)\n",
    "    Z = Z.reshape(xx0.shape)\n",
    "    y_pred = get_prediction(X_input, Y_input, weights, get_acc=False, model_type=model_type, predict=predict)\n",
    "\n",
    "  # Converting labels from {-1, 1} to {0, 1}\n",
    "  if class_label_01_form == 'on':\n",
    "    if np.shape(Y_input)[1] > 1:\n",
    "      neg_one_class_idxs = np.argwhere(y_pred == -1)\n",
    "      y_pred[neg_one_class_idxs[:, 0], neg_one_class_idxs[:, 1]] = 0\n",
    "      y_pred = y_pred.reshape((-1, np.shape(Y_input)[1]))\n",
    "    else:\n",
    "      neg_one_class_idxs = np.argwhere(y_pred == -1)[:, 0]\n",
    "      y_pred[neg_one_class_idxs] = 0\n",
    "      y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "  # Getting misclassfied points on train/val/test set\n",
    "  miscls_points = np.unique(np.argwhere(y_pred != Y_input)[:, 0])\n",
    "  \n",
    "  # Put the result into a color plot\n",
    "  plt.contour(xx0, xx1, Z, levels=[0], cmap='gray')\n",
    "\n",
    "  pred_data = np.hstack((X_input[:,:2], y_pred))\n",
    "  df = pd.DataFrame(data=pred_data, columns=[\"$X_0$\", \"$X_1$\", \"Y_\"+dataset_type+\"_pred\"])\n",
    "  sns.scatterplot(x=\"$X_0$\", y=\"$X_1$\", hue=\"Y_\"+dataset_type+\"_pred\", legend='full', data=df)\n",
    "  plt.scatter(X_input[miscls_points, 0], X_input[miscls_points, 1], s=150, cmap=\"Greens\", marker='x')\n",
    "  plt.title(dataset_type+' data')\n",
    "  plt.tight_layout();\n",
    "  plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aq9pilAkSDpc"
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "def mse_error(X_train, Y_train, THETA):\n",
    "  Y_PRED = np.dot(X_train, THETA)\n",
    "  m_sq_error = np.mean(0.5 * (Y_train - Y_PRED)**2)\n",
    "  return m_sq_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmwqMKwxMSfq"
   },
   "outputs": [],
   "source": [
    "@gif.frame\n",
    "def plot_convex_loss_and_predict_line(training_mse, weights, X_train, Y_train, Y_train_pred, i):\n",
    "\n",
    "  \"\"\" Method to plot Gradient Descent.\n",
    "\n",
    "  Parameters\n",
    "  -----------\n",
    "\n",
    "  training_mse: ndarray\n",
    "    List of training errors used to create decision boundary for every epoch/iteration.\n",
    "  \n",
    "  weights: ndarray\n",
    "    List of trained weights. This is used to create decision boundary for every epoch/iteration\n",
    "    and to see how the decision boundary changes.\n",
    "\n",
    "  X_train: ndarray (num_examples(rows) vs num_features(columns))\n",
    "    Input data which you would like to plot\n",
    "\n",
    "  Y_train: ndarray (num_examples(rows) vs num_outputs(columns))\n",
    "    Class labels\n",
    "\n",
    "  Y_train_pred: ndarray (num_examples(rows) vs num_outputs(columns))\n",
    "    Class labels\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=(15,10))\n",
    "  cmap = plt.get_cmap('cividis')\n",
    "  ax = fig.add_subplot(2, 1, 1, projection='3d')\n",
    "  \n",
    "  sns.set();\n",
    "  \n",
    "  weights = np.array(weights)\n",
    "  training_mse = np.array(training_mse) \n",
    "\n",
    "  weight_b_grid = np.linspace(weights[-1][0] - 20, weights[-1][0] + 20, 20) #intercept\n",
    "  weight_m_grid = np.linspace(weights[-1][1] - 40, weights[-1][1] + 40, 40) #slope\n",
    "\n",
    "  # Generating convex loss surfaces \n",
    "  B, M = np.meshgrid(weight_b_grid, weight_m_grid)\n",
    "  Y_train = Y_train.reshape(-1,)\n",
    "  zs = np.array([mse_error(X_train, Y_train, theta)\n",
    "                for theta in zip(np.ravel(B), np.ravel(M))])\n",
    "  Z = zs.reshape(M.shape)\n",
    "\n",
    "  ax.plot_surface(B, M, Z, rstride=1, cstride=1, color='blue', alpha=0.25)\n",
    "  ax.plot(weights[:i+1, 0], weights[:i+1, 1], training_mse[:i+1], markerfacecolor='r', markeredgecolor='r', marker='*', markersize=10, color='black')\n",
    "\n",
    "  # Plotting loss vs prediction\n",
    "  ax.set_title(\"Loss vs weights\")\n",
    "  ax.set_xlabel('weight(b)', labelpad=20)\n",
    "  ax.set_ylabel('weight(m)', labelpad=20)\n",
    "  ax.set_zlabel('Training loss', labelpad=10)\n",
    "  ax.view_init(elev=10, azim=25)\n",
    "\n",
    "  ax = fig.add_subplot(2, 1, 2)\n",
    "  ax.scatter(X_train[:, 1], Y_train, color=cmap(1), s=10)\n",
    "  ax.plot(X_train[:, 1], Y_train_pred[i], color='red', linewidth=0.2, label=\"Training-Prediction\")\n",
    "\n",
    "  ax.set_title(\"Linear Regression\")\n",
    "  ax.set_xlabel('Input (X_train)')\n",
    "  ax.set_ylabel('Y_train')\n",
    "  plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "utils_part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
