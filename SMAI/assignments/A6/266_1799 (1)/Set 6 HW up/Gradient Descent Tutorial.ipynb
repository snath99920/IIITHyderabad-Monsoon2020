{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7930,
     "status": "ok",
     "timestamp": 1596984057042,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "fo0ncTHEm0Gt",
    "outputId": "4d08b718-becd-43d5-9f12-ed16c8698790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gif in c:\\users\\aditya-pc\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.1.2 in c:\\users\\aditya-pc\\anaconda3\\lib\\site-packages (from gif) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have `gif` python package installed\n",
    "!pip install -U gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import utility functions\n",
    "\n",
    "We have provided a bunch of helper functions in the utils.ipynb notebook that allows for commonly used functions that are not central to the discussion of topic at hand, like plotting functions etc. Feel free to explore the functions and their implementations.\n",
    "\n",
    "The following cell imports the functions written in the utils notebook. You can ignore this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7905,
     "status": "ok",
     "timestamp": 1596984057052,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "_6NRlukDPKlg",
    "outputId": "41351a10-d770-4089-dbed-641587d0c6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        #print('searching: %s'%nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        #print('searching: %s' % nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        #print('Found %d cells'%len(nb.cells))\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "#  register the NotebookFinder with sys.meta_path\n",
    "print('running importer')\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkL4C3Oz17wr"
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42W7LYUI_kMY"
   },
   "source": [
    "Gradient descent is a first-order iterative optimization algorithm to find a local minima (or global minima in case of convex function) of a differentiable function. To achieve this, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\n",
    "\n",
    "Conveniently, the objective function of linear regression algorithm is strictly convex and differentiable. Hence, we can use gradient descent optimization to find optimal values of our hypothesis function. \n",
    "\n",
    "We revisit the equation to minimize the objective function of linear regression.\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\mathbf{w}} \\mathbf{J(w)} = \\min_{\\mathbf{w}} \\frac{1}{\\mathbf{N}} \\sum_\\mathbf{i=1}^\\mathbf{N} (\\mathbf{y_{i} - w^{\\top}x_i)^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "Our gradient descent update for weights is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w^{k+1}} \\leftarrow \\mathbf{w^k} - \\eta \\nabla{\\mathbf{J}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is our learning rate and,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla{\\mathbf{J}} = \\min_{\\mathbf{w}} \\frac{2}{\\mathbf{N}} \\sum_\\mathbf{i=1}^\\mathbf{N} (\\mathbf{y_{i} - w^{\\top}x_i)(-x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "We will look at Standard (Batch) Gradient Descent, Mini-batch, and Stochastic versions of Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8289,
     "status": "ok",
     "timestamp": 1596984057455,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "EFOxN1pUPfJ2",
    "outputId": "59f3ca6a-e2db-4b5e-bed4-51af7d1dd11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils_part2.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sb; sb.set();\n",
    "from utils_part2 import plot_convex_loss_and_predict_line, mse_error\n",
    "import gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8279,
     "status": "ok",
     "timestamp": 1596984057458,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "gprBlQapV1VU"
   },
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "# Generating regression dataset\n",
    "X_train, Y_train = make_regression(n_samples=num_samples, n_features=1, n_informative=1, noise=20, random_state=2017)\n",
    "\n",
    "# Adding bias as a column in X\n",
    "bias_ones = np.ones((num_samples, 1))\n",
    "X_train = np.hstack((bias_ones, X_train))\n",
    "Y_train = Y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8267,
     "status": "ok",
     "timestamp": 1596984057461,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "S_CrY0VzRWF8",
    "outputId": "24302da3-8c01-4ec5-deda-5db7024392f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: num_samples 100, num_features 2\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: num_samples {}, num_features {}\" .format((X_train.shape)[0], (X_train.shape)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8724,
     "status": "ok",
     "timestamp": 1596984057947,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "L2uxTneH_iqf",
    "outputId": "e7e14247-0b13-4904-e4fd-818dad7f05f4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7));\n",
    "plt.scatter(X_train[:, 1], Y_train, label='Point', color='green')\n",
    "plt.suptitle('X_train vs Y_train')\n",
    "plt.xlabel(\"X_train\")\n",
    "plt.ylabel(\"Y_train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8718,
     "status": "ok",
     "timestamp": 1596984057951,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "YOXZyxV4Ne77"
   },
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, num_epochs, pts_per_batch, learning_rate, gd_type):\n",
    "    training_errors = []\n",
    "    frames = []\n",
    "\n",
    "    # Initializing weights\n",
    "    w = np.array([-15, 40]) + np.random.rand(2)\n",
    "    w = w.reshape(-1, 1)\n",
    "\n",
    "    weights_history = []\n",
    "    Y_prediction = []\n",
    "    append_flag = False\n",
    "\n",
    "    # Getting number of batches/epochs to run\n",
    "    num_train_datapts = len(Y_train)\n",
    "    mini_batches = int(num_train_datapts/pts_per_batch)\n",
    "\n",
    "    # If mini_batches = num_training_datapoints, then standard GD is being executed.\n",
    "    # If mini_batches > 1 and < num_training_datapoints, and an integer, then mini-batch GD is being run.\n",
    "    # If mini_batches = 1, then SGD is being run.\n",
    "\n",
    "    num_batches = int(mini_batches * num_epochs)\n",
    "    print(f'Num_batches: {num_batches} Mini-batches: {mini_batches} Num_epochs: {num_epochs} Points_per_batch {pts_per_batch} train_point {num_train_datapts}')\n",
    "\n",
    "    # Do gradient descent for 'num_batches' batches (or mini-batches)\n",
    "    temp_mse_history = []\n",
    "    for i in range(1, num_batches+1):\n",
    "\n",
    "        if num_train_datapts == pts_per_batch:\n",
    "            # Sampling training data points for batch GD\n",
    "            X_input = X_train\n",
    "            Y_true = Y_train\n",
    "      \n",
    "        else:\n",
    "            # Sampling training data points for stochastic or mini-batch GD\n",
    "            train_idxs = np.random.permutation(num_train_datapts)\n",
    "            train_idxs = train_idxs[:pts_per_batch]\n",
    "            X_input = X_train[train_idxs]\n",
    "            Y_true = Y_train[train_idxs]\n",
    "      \n",
    "        y_pred = np.dot(X_input, w)\n",
    "        # Gradient of l2 loss w.r.t w\n",
    "        grad_w = -np.dot(X_input.T, Y_true - y_pred)\n",
    "\n",
    "        temp_mse = np.mean(0.5 * (Y_true - y_pred)**2)\n",
    "    \n",
    "        if num_train_datapts == pts_per_batch:\n",
    "          # Appending values during batch GD\n",
    "          mse = temp_mse\n",
    "          append_flag = True\n",
    "      \n",
    "        elif pts_per_batch != 1 and pts_per_batch != num_train_datapts:\n",
    "            temp_mse_history.append(temp_mse)\n",
    "            if ((i % mini_batches == 0 and mini_batches != 1) or i == 1) and pts_per_batch != 1:\n",
    "                y_pred = np.dot(X_train, w)\n",
    "                mse = np.mean(temp_mse_history)\n",
    "                temp_mse_history = []\n",
    "                append_flag = True\n",
    "\n",
    "        elif pts_per_batch == 1:\n",
    "            y_pred = np.dot(X_train, w)\n",
    "            mse = np.mean(0.5 * (Y_train - y_pred)**2)\n",
    "            append_flag = True\n",
    "     \n",
    "        # Appending errors, weights, and predictions only if an epoch is completed.\n",
    "        # This will help us in our visualization of GD.\n",
    "        if append_flag:\n",
    "            training_errors.append(mse)\n",
    "            weights_history.append(w)\n",
    "            Y_prediction.append(y_pred)\n",
    "            append_flag = False\n",
    "\n",
    "        # Update the weights\n",
    "        w -= (2*learning_rate/pts_per_batch) * grad_w\n",
    "        w = np.round(w, decimals=2)\n",
    "\n",
    "    return training_errors, weights_history, Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8706,
     "status": "ok",
     "timestamp": 1596984057953,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "Brcox848M1qD"
   },
   "outputs": [],
   "source": [
    "def generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='mini-batch'):\n",
    "    frames = []\n",
    "\n",
    "    for i in range(len(training_errors)):\n",
    "        frame = plot_convex_loss_and_predict_line(training_errors, weights_history, X_train, Y_train, Y_prediction, i)\n",
    "        frames.append(frame)\n",
    "    gif.save(frames, \"content/\"+gd_type+\"_GD.gif\", duration=500)\n",
    "    print('gifs generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fnpRV_VBvL2"
   },
   "source": [
    "Vanilla gradient descent, aka batch gradient descent, calculates the gradient of the loss function w.r.t to the parameters $w$ for all the training examples. We need to only make one update per epoch. However, for a large dataset, batch GD can be slow and sometimes intractable if the datasets don't fit in memory. Also, we can't update our model online, i.e., with new examples on-the-fly. The pseudo code is as follows:\n",
    "\n",
    "For a dataset $D = (x_i, y_i)_i^m$ of $m$ training examples, \n",
    "\n",
    "1.  Initializing weights and biases.\n",
    "2.  For every epoch $\\in [1,...., \\text{max_epochs}]$:\n",
    "     * shuffle D to prevent cycles\n",
    "     * compute prediction $y_{\\text{prediction}} = activation(X_{\\text{input}} \\times weights)$\n",
    "     * compute loss $L = (y_{\\text{true}}, y_{\\text{predicted}})$\n",
    "     * compute gradients $\\Delta \\mathbf{w} = - \\nabla_{Loss} \\mathbf{w}$\n",
    "     * update parameters $\\mathbf{w} :=\\mathbf{w}+\\Delta \\mathbf{w}$\n",
    "\n",
    "Batch GD is guaranteed to converge to the global minima for convex loss surfaces and to a local minima for non-convex surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21846,
     "status": "ok",
     "timestamp": 1596984071119,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "fJxjnF0oMeh7",
    "outputId": "d6ab2cf9-3f69-4d5e-889c-d34c4e9b1fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_batches: 20 Mini-batches: 1 Num_epochs: 20 Points_per_batch 100 train_point 100\n",
      "gifs generated\n"
     ]
    }
   ],
   "source": [
    "# Batch GD\n",
    "training_errors, weights_history, Y_prediction = train(X_train, Y_train, num_epochs=20, pts_per_batch=100, learning_rate=0.1, gd_type='batch')\n",
    "\n",
    "generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0ZVnp9jsLEc"
   },
   "source": [
    "Visualizing Batch Gradient Descent\n",
    "\n",
    "![Batch GD](https://drive.google.com/uc?id=1OSGBdrmU6a5pCl5f0VOnmon05V2N7Ff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63330,
     "status": "ok",
     "timestamp": 1596984765073,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "9ecPhD7UUxHp",
    "outputId": "76a7bc4c-1c76-4595-cab5-1ecdcde25a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_batches: 100 Mini-batches: 100 Num_epochs: 1 Points_per_batch 1 train_point 100\n",
      "gifs generated\n"
     ]
    }
   ],
   "source": [
    "# Stochastic GD\n",
    "training_errors, weights_history, Y_prediction = train(X_train, Y_train, num_epochs=1, pts_per_batch=1, learning_rate=0.1, gd_type='SGD')\n",
    "generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5GcRVtBGFvR"
   },
   "source": [
    "In Stochastic GD, parameter updates are performed for every training example. Due to frequent updates, this leads to high variance that causes the loss function to fluctuate heavily.\n",
    "\n",
    "These fluctuations can sometimes help to find better local minima. On the other hand, these fluctuations can lead to longer convergence time.\n",
    "\n",
    "For a dataset $D = (x_i, y_i)_i^m$ of $m$ training examples, \n",
    "\n",
    "1.  Initializing weights and biases.\n",
    "2.  For every epoch $\\in [1,...., \\text{max_epochs}]$:\n",
    ">2.1  For iteration $\\text{itr} \\in [1,....., m]$: \n",
    "     * Draw random examples with replacement: $(x_i, y_i) \\in D$\n",
    "     * compute prediction $y_i^{\\text{prediction}} = activation(x_i \\times weights)$\n",
    "     * compute loss $L_i = (y_i^{\\text{true}}, y_i^{\\text{predicted}})$\n",
    "     * compute gradients $\\Delta \\mathbf{w} = - \\nabla_{Loss_i} \\mathbf{w}$\n",
    "     * update parameters $\\mathbf{w} :=\\mathbf{w}+\\Delta \\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNyQ81jK0qJV"
   },
   "source": [
    "Visualizing Stochastic Gradient Descent\n",
    "\n",
    "![SGD](https://drive.google.com/uc?id=1gdelm-z5cjNR1WkdEIiC_VOYCcpbpOlx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V18l7xQlDr0Z"
   },
   "source": [
    "Minibatch GD offers the best of both worlds, i.e., a trade-off between the stochastic GD that perform updates based on a single training example and Batch GD which takes all of the dataset at once. \n",
    "\n",
    "In mini-batch, update is performed for every mini-batch of k training examples. Hence, we would have to take more updates per epoch as compared to batch GD.\n",
    "\n",
    "The updates reduces the variances of parameter updates leading to a more stable convergence. By using highly optimized matrix computation from state-of-the-art machine learning libraries, mini-batch is typically the algorithm of choice when training a neural network.\n",
    "\n",
    "For a dataset $D = (x_i, y_i)_i^m$ of $m$ training examples,\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>Initializing weights and biases.</li>\n",
    "<li>For every epoch $\\in [1,...., \\text{max_epochs}]$:</li>\n",
    "<ul>\n",
    "  <li>For iteration $i \\in [1,....., n]$: (where n is the mini-batch size)</li>\n",
    "  <ul>\n",
    "    <li>Draw random examples with replacement: $(x_i, y_i) \\in D$</li>\n",
    "  </ul>\n",
    "  <li>compute prediction $y_i^{\\text{prediction}} = activation(x_i \\times weights)$</li>\n",
    "  <li>compute loss $L_i = (y_i^{\\text{true}}, y_i^{\\text{predicted}})$</li>\n",
    "  <li>compute gradients $\\Delta \\mathbf{w} = - \\nabla_{L_i} \\mathbf{w}$</li>\n",
    "  <li>update parameters $\\mathbf{w} :=\\mathbf{w}+\\Delta \\mathbf{w}$</li>\n",
    "  </ul>\n",
    "</ol>\n",
    "  \n",
    "\n",
    "One can also draw examples without replacement. Drawing examples w/o replacement is more commonly used in modern deep learning libraries. However, we shall look at mini-batch GD where we draw samples at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98283,
     "status": "ok",
     "timestamp": 1596984147605,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "rI0NNEtpUvig",
    "outputId": "4882b54f-c86e-4c3f-819e-1819d52052de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_batches: 100 Mini-batches: 5 Num_epochs: 20 Points_per_batch 20 train_point 100\n",
      "gifs generated\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch GD\n",
    "training_errors, weights_history, Y_prediction = train(X_train, Y_train, num_epochs=20, pts_per_batch=20, learning_rate=0.1, gd_type='mini-batch')\n",
    "generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='mini-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5qoTFkhlwLL"
   },
   "source": [
    "Visualizing mini-batch Gradient Descent\n",
    "\n",
    "![mini-batch](https://drive.google.com/uc?id=1Lp1tDUS1cid-TUc37zhc5PI9VdYmmzjc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w56I_q3UOzfJ"
   },
   "source": [
    "# Visualizing the effect of learning rate on GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG4a4J0LAa1D"
   },
   "source": [
    "Now that we have seen the effect of changing the number of batches in a gradient descent, we look at the effect of different learning rates, another important factor that actively influences gradient descent. Previously, we had used 0.1 as our learning rate to train the linear regression model. Let's use Batch gradient descent as an example with lr rate $0.005$ and $1.1. You can try other values for other GD types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110760,
     "status": "ok",
     "timestamp": 1596984160107,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "cGM2b8PbPBrK",
    "outputId": "bf8a9c07-3abb-43ad-d35d-442dcac94612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_batches: 20 Mini-batches: 1 Num_epochs: 20 Points_per_batch 100 train_point 100\n",
      "gifs generated\n"
     ]
    }
   ],
   "source": [
    "# Batch GD\n",
    "training_errors, weights_history, Y_prediction = train(X_train, Y_train, num_epochs=20, pts_per_batch=100, learning_rate=0.005, gd_type='batch')\n",
    "generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='batch-small-lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18ju1zgmaPZJ"
   },
   "source": [
    "![mini-batch-small-lr](https://drive.google.com/uc?id=1gGa7gcwHSCOSUqleCuQKyY1ur1T55W7y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 119531,
     "status": "ok",
     "timestamp": 1596984168914,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "lEJ9DNHoPCOr",
    "outputId": "261212dc-8d04-4b42-d03c-cd7d18e91bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_batches: 15 Mini-batches: 1 Num_epochs: 15 Points_per_batch 100 train_point 100\n",
      "gifs generated\n"
     ]
    }
   ],
   "source": [
    "# Batch GD\n",
    "training_errors, weights_history, Y_prediction = train(X_train, Y_train, num_epochs=15, pts_per_batch=100, learning_rate=1.1, gd_type='batch')\n",
    "generate_GD_gifs(training_errors, weights_history, X_train, Y_train, Y_prediction, gd_type='batch-large-lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxcuV8tyaGeF"
   },
   "source": [
    "![Batch-large-lr](https://drive.google.com/uc?id=1cMrXF1Edjrg0ZA3oj9YEaRpfntq8_bq9)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GradientDescent_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
