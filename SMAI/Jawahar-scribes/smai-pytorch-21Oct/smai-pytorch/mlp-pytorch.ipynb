{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "A feed forward neural network with multiple layers and each layer having dense connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: MNIST Digit Classification\n",
    "\n",
    "### Model\n",
    "![alt text](images/MLP.png \"Title\")\n",
    "\n",
    "### Layer Diagram\n",
    "![alt text](images/LayerDiag.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Dataset $\\rightarrow$ Building Computational Graph (Model) $\\rightarrow$ Define Loss and Optimizer $\\rightarrow$ Training (Forward$\\rightarrow$Backward$\\rightarrow$Parameter update) $\\rightarrow$ Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1477b391b7f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as weight_init\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "\n",
    "#Loading the train set file\n",
    "train_dataset = datasets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=preprocess,  \n",
    "                            download=True)\n",
    "#Loading the test set file\n",
    "test_dataset = datasets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=preprocess)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].size())\n",
    "\n",
    "#Plotting\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_dataset[110][0].squeeze(),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining DataLoader\n",
    "- Batching the data\n",
    "- Shuffling the data\n",
    "- Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([100, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([100]) type: torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataloader\n",
    "batch_size = 100\n",
    "\n",
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "#loading the test dataset\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "\n",
    "#Plotting 10 digits\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "input_size = 784\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 25\n",
    "learning_rate = 0.01\n",
    "momentum_rate = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #Weight Initialization\n",
    "        for m in self.modules():\n",
    "          if isinstance(m,nn.Linear):\n",
    "            weight_init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size, hidden_size, num_classes)\n",
    "\n",
    "#Other functions\n",
    "def loss_plot(losses):\n",
    "    max_epochs = len(losses)\n",
    "    times = list(range(1, max_epochs+1))\n",
    "    plt.figure(figsize=(30, 7))\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"cross-entropy loss\")\n",
    "    return plt.plot(times, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum_rate)\n",
    "\n",
    "##RMSProp\n",
    "# torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "##Adam\n",
    "# torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "##Adagrad\n",
    "# torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA  True\n",
      "Epoch [1/25] Loss:1.6566\n",
      "Epoch [2/25] Loss:1.5404\n",
      "Epoch [3/25] Loss:1.5248\n",
      "Epoch [4/25] Loss:1.5154\n",
      "Epoch [5/25] Loss:1.5089\n",
      "Epoch [6/25] Loss:1.5037\n",
      "Epoch [7/25] Loss:1.4994\n",
      "Epoch [8/25] Loss:1.4961\n",
      "Epoch [9/25] Loss:1.4929\n",
      "Epoch [10/25] Loss:1.4904\n",
      "Epoch [11/25] Loss:1.4881\n",
      "Epoch [12/25] Loss:1.4863\n",
      "Epoch [13/25] Loss:1.4846\n",
      "Epoch [14/25] Loss:1.4831\n",
      "Epoch [15/25] Loss:1.4819\n",
      "Epoch [16/25] Loss:1.4807\n",
      "Epoch [17/25] Loss:1.4797\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Using CUDA ', use_cuda)\n",
    "\n",
    "#Transfer to GPU device if available\n",
    "net = net.to(device)\n",
    "net\n",
    "\n",
    "# Training\n",
    "epochLoss = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    cntr = 1\n",
    "    # For each batch of images in train set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Initialize gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (this calls the \"forward\" function within Net)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Find the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass (Find the gradients of all weights using the loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using the optimizer\n",
    "        # For e.g.: w = w - (delta_w)*lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        cntr += 1\n",
    "    \n",
    "    print('Epoch [%d/%d] Loss:%.4f'%(epoch+1, num_epochs, total_loss/cntr) )\n",
    "    epochLoss.append(total_loss/cntr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = loss_plot(epochLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# For each batch of images in test set\n",
    "with torch.no_grad():\n",
    "  for images, labels in test_loader:\n",
    "\n",
    "      # Get the images\n",
    "      images = images.view(-1, 28*28)\n",
    "\n",
    "      images = images.to(device)\n",
    "\n",
    "      # Find the output by doing a forward pass through the network\n",
    "      outputs = net(images)\n",
    "\n",
    "      # Find the class of each sample by taking a max across the probabilities of each class\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Goals\n",
    "- Varying #neurons, #layers\n",
    "- Activation functions\n",
    "- Loss functions\n",
    "- Learning Rates\n",
    "- Optimizers\n",
    "- Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rates\n",
    "![](images/LR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "Source: Visualization of stochastic optimizers by Alec Radford\n",
    "![](http://2.bp.blogspot.com/-q6l20Vs4P_w/VPmIC7sEhnI/AAAAAAAACC4/g3UOUX2r_yA/s400/s25RsOr%2B-%2BImgur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization (Glorot et.al JMLR’10)\n",
    "- Scale of initialization is dependent on the number of input and output neurons.\n",
    "- Initial weights are sampled from $\\mathcal{N}(0,var(w))$, where $var(w)=\\frac{2}{n_{in}+n_{out}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "- Dataset and Dataloader\n",
    "- Model definition and interconnections\n",
    "- Loss function\n",
    "- Forward Pass\n",
    "- Backward Pass\n",
    "- Loss function\n",
    "- Parameter update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
