{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMDxcecjIOKi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "LrvnXK84IP16",
    "outputId": "cc8f94a9-0db7-4e55-a23c-806711a640cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI13LZ8dISL5"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzsmHhIUIT5f"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    # print(tokenizer.sequences_to_text(corpus_tokenized))\n",
    "    # print(tokenizer.word_index)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return tokenizer.word_index, corpus_tokenized, V\n",
    "\n",
    "\n",
    "def initialize(V, N):\n",
    "    np.random.seed(100)\n",
    "    W1 = np.random.rand(V, N)\n",
    "    W2 = np.random.rand(N, V)\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "def to_categorical(y, num_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    # print(categorical)\n",
    "    return categorical\n",
    "\n",
    "\n",
    "def corpus2ContextnCenter(corpus_tokenized, V, ws):\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        # print(L)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            center = []\n",
    "            for i in range(index - ws, index + ws + 1):\n",
    "              if 0 <= i < L and i != index:\n",
    "                contexts = contexts + [words[i]-1]\n",
    "            center.append(word-1)\n",
    "            # x has shape c x V where c is size of contexts\n",
    "            x = to_categorical(contexts, V)\n",
    "            # y has shape k x V where k is number of center words\n",
    "            y = to_categorical(center, V)\n",
    "            yield (x, y)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvcks3yyIVwB"
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'ws': 2,\n",
    "    'n': 10,\n",
    "    'epochs': 2,\n",
    "    'lr': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KX3w3uimIXih"
   },
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "  \n",
    "    def __init__(self, corpus=''):\n",
    "        self.window = settings['ws']\n",
    "        self.N = settings['n']\n",
    "        self.corpus = corpus\n",
    "        self.eta = settings['lr']\n",
    "\n",
    "    \n",
    "    def skipgram(self, context, x, W1, W2, loss):\n",
    "        h = np.matmul(W1.T, x.T)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        e = np.outer(y_pred,np.array([1]*context.shape[0]))-context.T\n",
    "\n",
    "        dW2 = np.outer(h, np.sum(e, axis=1))\n",
    "        dW1 = np.outer(x, np.dot(W2, np.sum(e, axis=1)))\n",
    "\n",
    "        new_W1 = W1 - settings['lr'] * dW1\n",
    "        new_W2 = W2 - settings['lr'] * dW2\n",
    "\n",
    "        loss += - np.sum([u[label.T == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss\n",
    "\n",
    "\n",
    "    def run(self, corpus_tokenized, V):\n",
    "        # corpus_tokenized, V = tokenize(self.corpus)\n",
    "        # print(corpus_tokenized)\n",
    "        # print(V)\n",
    "        W1, W2 = initialize(V, self.N)\n",
    "\n",
    "        loss_vs_epoch = []\n",
    "        for e in range(settings['epochs']):\n",
    "            loss = 0.\n",
    "            for context, center in corpus2ContextnCenter(corpus_tokenized, V, self.window):\n",
    "                W1, W2, loss = self.skipgram(context, center, W1, W2, loss)\n",
    "            loss_vs_epoch.append(loss)\n",
    "\n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aq6zHVoIZaz"
   },
   "outputs": [],
   "source": [
    "def predict(x, W1, W2):\n",
    "        h = np.mean([np.matmul(W1.T, xx) for xx in x], axis=0)\n",
    "        u = np.dot(W2.T, h)\n",
    "        return softmax(u)\n",
    "\n",
    "def cosine_distance(word, V, W1, myVocab):\n",
    "  # print(word)\n",
    "  # word = float(word)\n",
    "  cosine_d = {}\n",
    "  for i in range(V):\n",
    "    vocab_sc = W1[i]\n",
    "    # vocab_sc = vocab_sc.astype(float)\n",
    "    # word = word.astype(float)\n",
    "    a = np.dot(word, vocab_sc)\n",
    "    b = np.linalg.norm(word) * np.linalg.norm(vocab_sc)\n",
    "    # word = map(lambda x: float(x), word)\n",
    "    # vocab_sc = map(lambda x: float(x), vocab_sc)\n",
    "    theta = a/b\n",
    "    # print(V)\n",
    "    # print(theta)\n",
    "    key_word = ''\n",
    "    for key, value in myVocab.items():\n",
    "      if i == value:\n",
    "        key_word = key\n",
    "        break\n",
    "    \n",
    "    cosine_d[key_word] = theta\n",
    "\n",
    "    # print(cosine_d)\n",
    "  sorted_list = sorted(cosine_d.items(), key=lambda kv: kv[1], reverse=True)\n",
    "  for key, sim in sorted_list[1:11]:\n",
    "    print(\"{} : {}\".format(key, sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjmCy5WMIbeB"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # corpus = \"We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \"trucker\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that's just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!I'm a professional OTR truck driver, and I bought a TND 700 at a truck stop hoping to make my life easier.  Rand McNally, are you listening?First thing I did after charging it was connect it to my laptop and install the software and then attempt to update it.  The software detected a problem with my update and wanted my home address so I could be sent a patch on an SD card.  Hello?  I don't think I'm all that unusual; my home address is a PO box that a friend checks weekly and that I might get to check every six months or so.  I live in my truck and at truck stops.  If you need to make a patch available on an SD card then you should send the SD cards to the truck stops where the devices are sold.  I ran the update program multiple times until the program said that the TND 700 was completely updated.I programmed in the height (13'6\"), the length (53') and the weight (80,000#) of my rig and told it that I preferred highways.  I was parked at a truck stop in the Cincinnati OH area.  My next pickup was about 15 miles down the same freeway but on the other side of it a couple of blocks.  My cell phone GPS (Sprint) said to get on the freeway to get to my pickup.  The TND 700 routed me thru 23 miles of residential streets before finally getting me to my pickup.  Very exciting, especially since every time I refused to turn down a street posted \"No Trucks\" the TND 700 took almost 5 minutes to figure a re-route, and it happened multiple times on that short trip.I decided to give it another chance.  After my pickup on the north side of Cincinnati just off of I-75 I needed to head to Phoenix AZ via I-71.  Easy route is to just hop on I-75 and drive west and south to the intersection of I-71.  Indeed, that is what my cell phone advised.  The TND 700, however, wanted to route me over surface streets across the city and pick up I-75 on the other side of the city.  I turned it off and the next time I passed a truck stop of the same chain I purchased it at I returned it and got my money back.I then spent $30 on a cheap printer.  Now I take a minute to set up my route on Google and print it out.  Hasn't gotten me lost yet over several cross country trips.\"\n",
    "    import json\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "\n",
    "    fileIn = open('/content/drive/My Drive/Dataset/reviews_Electronics_5.json')\n",
    "    fileOut1 = open('/content/drive/My Drive/Dataset/weights1skipgram.txt', 'w+')\n",
    "    fileOut2 = open('/content/drive/My Drive/Dataset/weights2skipgram.txt', 'w+')\n",
    "    lines = fileIn.readlines()\n",
    "\n",
    "    corpus = ''\n",
    "    numWords = 0\n",
    "    for line in lines:\n",
    "      if numWords < 50000:\n",
    "        data = json.loads(line)\n",
    "        for val in data['reviewText'].split('.'):\n",
    "          sent = re.findall(\"[A-Za-z]+\", val)\n",
    "          line = ''\n",
    "          for words in sent:\n",
    "            if len(words) > 1 and words not in stop_words:\n",
    "              line += ' ' + words\n",
    "              numWords += 1\n",
    "          corpus += line\n",
    "      # # corpus = corpus+data['reviewText']\n",
    "\n",
    "    # # print(corpus)\n",
    "    # corpus = \"I like playing football with my friends\"\n",
    "    \n",
    "    w2v = Word2Vec(corpus=corpus)\n",
    "\n",
    "    # print(w2v.corpus)\n",
    "\n",
    "    myVocab, corpus_tokenized, V = tokenize([corpus])\n",
    "    # print(corpus_tokenized)\n",
    "    # print(V)\n",
    "    # W1, W2 = initialize(V, settings['n'])\n",
    "    vocab_words = list(myVocab.keys())\n",
    "\n",
    "    W1, W2 = w2v.run(corpus_tokenized, V)\n",
    "\n",
    "    fileOut1.write(str(W1))\n",
    "    fileOut2.write(str(W2))\n",
    "\n",
    "    # print(W1)\n",
    "    # print(W2)\n",
    "    # print(myVocab)\n",
    "    word = \"camera\"\n",
    "    # print(myVocab[word])\n",
    "    index = myVocab[word]\n",
    "    x = np.zeros(V, dtype='int')\n",
    "    x[index-1] = 1\n",
    "    word_vector = W1[index-1]\n",
    "    # print(word_vector)\n",
    "    y_pred = predict([x], W1, W2)\n",
    "    print(y_pred)\n",
    "\n",
    "    cosine_distance(word_vector, V, W1, myVocab)\n",
    "\n",
    "    word = \"great\"\n",
    "    # print(myVocab[word])\n",
    "    index = myVocab[word]\n",
    "    x = np.zeros(V, dtype='int')\n",
    "    x[index-1] = 1\n",
    "    word_vector = W1[index-1]\n",
    "    # print(word_vector)\n",
    "    y_pred = predict([x], W1, W2)\n",
    "    print(y_pred)\n",
    "\n",
    "    word = \"trucker\"\n",
    "    # print(myVocab[word])\n",
    "    index = myVocab[word]\n",
    "    x = np.zeros(V, dtype='int')\n",
    "    x[index-1] = 1\n",
    "    word_vector = W1[index-1]\n",
    "    # print(word_vector)\n",
    "    y_pred = predict([x], W1, W2)\n",
    "    print(y_pred)\n",
    "\n",
    "    word = \"working\"\n",
    "    # print(myVocab[word])\n",
    "    index = myVocab[word]\n",
    "    x = np.zeros(V, dtype='int')\n",
    "    x[index-1] = 1\n",
    "    word_vector = W1[index-1]\n",
    "    # print(word_vector)\n",
    "    y_pred = predict([x], W1, W2)\n",
    "    print(y_pred)\n",
    "\n",
    "    word = \"tripod\"\n",
    "    # print(myVocab[word])\n",
    "    index = myVocab[word]\n",
    "    x = np.zeros(V, dtype='int')\n",
    "    x[index-1] = 1\n",
    "    word_vector = W1[index-1]\n",
    "    # print(word_vector)\n",
    "    y_pred = predict([x], W1, W2)\n",
    "    print(y_pred)\n",
    "    \n",
    "\n",
    "    # print(y_pred)\n",
    "\n",
    "    # print(loss_vs_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "KA3e32EGId01",
    "outputId": "49029072-878a-45bb-820d-5ca71d6b00a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.58324590e-02 5.48189239e-03 1.55487489e-02 ... 4.72645341e-05\n",
      " 4.48262438e-05 6.45275033e-05]\n",
      "even : 0.9948375000603757\n",
      "ed : 0.9855832933897262\n",
      "player : 0.9847006231262169\n",
      "volume : 0.9822266652058244\n",
      "core : 0.9817757948541198\n",
      "quirks : 0.9817535035112914\n",
      "end : 0.9812382908058027\n",
      "flipping : 0.9793805854919142\n",
      "life : 0.9762058863638781\n",
      "brave : 0.9755448026073842\n",
      "[4.84651551e-02 1.28742394e-02 2.52897338e-02 ... 4.75477267e-05\n",
      " 2.72553561e-05 3.35519070e-05]\n",
      "[2.13710274e-02 5.88833156e-03 1.90667475e-02 ... 4.75755026e-05\n",
      " 5.82023025e-05 2.84846419e-05]\n",
      "[1.93926965e-02 7.77952580e-03 9.61556897e-03 ... 5.15534149e-05\n",
      " 5.44087681e-05 7.67880575e-05]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d2775e5869a3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nikon\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# print(myVocab[word])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyVocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nikon'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
